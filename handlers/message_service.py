import logging
import re
import time
from dataclasses import dataclass
from typing import Awaitable, Callable, List, Optional

from config import BOT_CONFIG
from handlers.commands import MODELS_HINT_TEXT
from services.consilium import (
    extract_prompt_from_consilium_message,
    format_consilium_results,
    generate_consilium_responses,
    parse_models_from_message,
    select_default_consilium_models,
)
from services.generation import CATEGORY_TITLES, build_models_messages, generate_image, generate_text
from services.memory import (
    add_message,
    get_history,
    get_preferred_model,
    get_routing_mode,
    get_show_response_header,
    set_preferred_model,
    set_routing_mode,
    set_show_response_header,
)
from services.router import route_request
from services.web_search import search_web

logger = logging.getLogger(__name__)


@dataclass
class MessageProcessingRequest:
    text: str
    chat_id: str
    user_id: str
    bot_username: str | None = None
    username: str | None = None


@dataclass
class MessageResponse:
    text: str | None = None
    photo_url: str | None = None
    parse_mode: str | None = None


@dataclass
class RoutedRequest:
    request_type: str
    content: str
    suggested_models: List[str]
    model: str | None
    category: str | None
    use_context: bool
    reason: str | None
    user_routing_mode: str


_ROUTING_RULES_KEYWORDS = {
    "—Ä–æ—É—Ç–∏–Ω–≥ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏",
    "—Ä–æ—É—Ç–∏–Ω–≥ –ø—Ä–∞–≤–∏–ª–∞–º–∏",
    "routing rules",
    "routing algorithms",
    "routing algo",
}

_ROUTING_LLM_KEYWORDS = {
    "—Ä–æ—É—Ç–∏–Ω–≥ –ª–ª–º",
    "—Ä–æ—É—Ç–∏–Ω–≥ llm",
    "routing llm",
    "routing ai",
}

_ROUTING_STATUS_KEYWORDS = {
    "–∫–∞–∫–æ–π —Ä–æ—É—Ç–∏–Ω–≥",
    "—Ä–µ–∂–∏–º —Ä–æ—É—Ç–∏–Ω–≥–∞",
    "routing mode",
}

_HEADER_DISABLE_KEYWORDS = {
    "—Å–ø—Ä—è—á—å —à–∞–ø–∫—É",
    "—Å–∫—Ä–æ–π —à–∞–ø–∫—É",
    "—Å–∫—Ä—ã—Ç—å —à–∞–ø–∫—É",
    "–≤—ã–∫–ª—é—á–∏ —à–∞–ø–∫—É",
    "–æ—Ç–∫–ª—é—á–∏ —à–∞–ø–∫—É",
    "—É–±–µ—Ä–∏ —à–∞–ø–∫—É",
    "–±–µ–∑ —à–∞–ø–∫–∏",
    "—Å–∫—Ä–æ–π —Ç–µ—Ö—à–∞–ø–∫—É",
}

_HEADER_ENABLE_KEYWORDS = {
    "–≤–∫–ª—é—á–∏ —à–∞–ø–∫—É",
    "–ø–æ–∫–∞–∑—ã–≤–∞–π —à–∞–ø–∫—É",
    "–≤–µ—Ä–Ω–∏ —à–∞–ø–∫—É",
    "–ø–æ–∫–∞–∂–∏ —à–∞–ø–∫—É",
    "–≤–∫–ª—é—á–∏ —Ç–µ—Ö—à–∞–ø–∫—É",
    "—Ç–µ—Ö—à–∞–ø–∫–∞ –≤–∫–ª",
}

_MODEL_PREFERENCE_PATTERN = re.compile(r"^–æ—Ç–≤–µ—á–∞–π\s+–≤—Å–µ–≥–¥–∞\s+(?:—Å|—á–µ—Ä–µ–∑)\s+(.+)$", re.IGNORECASE)
_MODEL_PREFERENCE_RESET_KEYWORDS = {
    "–æ—Ç–≤–µ—á–∞–π –∫–∞–∫ –æ–±—ã—á–Ω–æ",
    "–∏—Å–ø–æ–ª—å–∑—É–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –º–æ–¥–µ–ª—å",
    "—Å–±—Ä–æ—Å—å –º–æ–¥–µ–ª—å",
    "–æ—Ç–∫–ª—é—á–∏ –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é",
}


def _resolve_model_alias(model_text: str) -> str | None:
    models = BOT_CONFIG.get("MODELS", {})
    normalized = model_text.strip().lower()

    if normalized in models:
        return models[normalized]

    for value in models.values():
        if normalized == value.lower():
            return value

    return None


def _normalize_routing_choice(text: str) -> str | None:
    normalized = text.strip().lower()
    if normalized in _ROUTING_RULES_KEYWORDS:
        return "rules"
    if normalized in _ROUTING_LLM_KEYWORDS:
        return "llm"
    return None


def _is_routing_status_request(text: str) -> bool:
    return text.strip().lower() in _ROUTING_STATUS_KEYWORDS


def _normalize_header_toggle(text: str) -> bool | None:
    normalized = text.strip().lower()
    if normalized in _HEADER_DISABLE_KEYWORDS:
        return False
    if normalized in _HEADER_ENABLE_KEYWORDS:
        return True
    return None


def _format_response_header(
    routing_mode: str | None, context_info: dict | None, model: str | None
) -> str | None:
    parts: list[str] = []

    if routing_mode:
        routing_label = "–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π" if routing_mode == "rules" else "LLM"
        parts.append(f"üîÄ –†–æ—É—Ç–∏–Ω–≥: {routing_label}")

    if context_info:
        tokens = context_info.get("usage_tokens")
        chars = context_info.get("usage_chars")
        limit = context_info.get("context_limit")

        context_chunks: list[str] = []
        if tokens and limit:
            context_chunks.append(f"{tokens}/{limit} —Ç")
        elif tokens:
            context_chunks.append(f"{tokens} —Ç")

        if chars:
            context_chunks.append(f"{chars} —Å–∏–º–≤")

        if context_chunks:
            parts.append(f"üì¶ –ö–æ–Ω—Ç–µ–∫—Å—Ç: {' ‚Ä¢ '.join(context_chunks)}")

        trimmed = context_info.get("trimmed_from_context")
        if trimmed:
            parts.append(f"‚úÇÔ∏è –û–±—Ä–µ–∑–∞–Ω–æ: {trimmed}")

        if context_info.get("summary_text"):
            parts.append("üßæ –°–∞–º–º–∞—Ä–∏ –∏—Å—Ç–æ—Ä–∏–∏")

        warnings = context_info.get("warnings") or []
        if warnings:
            parts.append(f"‚ö†Ô∏è {warnings[0]}")

    if model:
        parts.append(f"ü§ñ –ú–æ–¥–µ–ª—å: {model}")

    return " ‚Ä¢ ".join(parts) if parts else None


def _build_context_guard_notices(context_info: dict | None) -> list[str]:
    if not context_info:
        return []

    notices = []
    if context_info.get("summary_text"):
        notices.append("‚ö†Ô∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω ‚Äî –¥–µ–ª–∞—é —Å–∞–º–º–∞—Ä–∏ –∏—Å—Ç–æ—Ä–∏–∏.")
    elif context_info.get("trimmed_from_context"):
        notices.append("‚ö†Ô∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω ‚Äî —Å–∫—Ä—ã–≤–∞—é —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –∑–∞–ø—Ä–æ—Å–∞.")

    for warn in context_info.get("warnings", []):
        notices.append(f"‚ÑπÔ∏è {warn}")

    return notices


async def get_capabilities() -> list[str]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö."""
    try:
        capabilities = await build_models_messages(
            ["free", "large_context", "specialized", "paid"],
            header="ü§ñ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\n\n",
            max_items_per_category=20,
        )

        if not capabilities:
            return ["–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö."]

        instructions = "üí° –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:\n"
        instructions += f"‚Ä¢ –ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å - –æ—Ç–≤–µ—á—É —á–µ—Ä–µ–∑ {BOT_CONFIG['DEFAULT_MODEL']}\n"
        instructions += "‚Ä¢ –£–∫–∞–∂–∏ –º–æ–¥–µ–ª—å –≤ –Ω–∞—á–∞–ª–µ ('chatgpt —Ä–∞—Å—Å–∫–∞–∂–∏ –æ –ø–æ–≥–æ–¥–µ')\n"
        instructions += "‚Ä¢ –ò–ª–∏ –≤ –∫–æ–Ω—Ü–µ ('—Ä–∞—Å—Å–∫–∞–∂–∏ –æ –ø–æ–≥–æ–¥–µ —á–µ—Ä–µ–∑ claude')\n"
        instructions += "‚Ä¢ –î–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–π '–Ω–∞—Ä–∏—Å—É–π' –∏–ª–∏ '—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫–∞—Ä—Ç–∏–Ω–∫—É'"

        if len(capabilities[-1] + instructions) > 3000:
            capabilities.append(instructions)
        else:
            capabilities[-1] += instructions

        return capabilities
    except Exception as e:
        logger.error(f"Error getting capabilities: {str(e)}")
        return ["–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö."]


async def send_models_by_request(
    order: list[str],
    header: str,
    max_items: int | None = 20,
) -> List[MessageResponse]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""

    parts = await build_models_messages(order, header=header, max_items_per_category=max_items)
    if not parts:
        return [MessageResponse(text="–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")]

    return [MessageResponse(text=part) for part in parts]


def _build_routed_request(
    decision,
    effective_text: str,
    user_routing_mode: str,
) -> RoutedRequest:
    request_type = decision.action or "text"
    content = decision.prompt or effective_text
    suggested_models = decision.target_models or []
    model = suggested_models[0] if suggested_models else None
    category = decision.category
    use_context = decision.use_context
    reason = decision.reason

    if request_type == "search" and not content:
        request_type = "search_previous"

    if request_type == "models_category" and category:
        content = category

    if request_type == "text" and len(suggested_models) > 1:
        request_type = "consilium"

    return RoutedRequest(
        request_type=request_type,
        content=content,
        suggested_models=suggested_models,
        model=model,
        category=category,
        use_context=use_context,
        reason=reason,
        user_routing_mode=user_routing_mode,
    )


async def execute_routed_request(
    request: MessageProcessingRequest,
    routed: RoutedRequest,
    ack_callback: Optional[Callable[[], Awaitable[None]]] = None,
) -> List[MessageResponse]:
    responses: List[MessageResponse] = []

    chat_id = request.chat_id
    user_id = request.user_id
    preferred_model = get_preferred_model(chat_id, user_id)
    show_response_header = get_show_response_header(chat_id, user_id)

    request_type = routed.request_type
    content = routed.content
    suggested_models = routed.suggested_models
    model = routed.model
    use_context = routed.use_context

    async def notify_model_switch(failed_model: str, next_model: str, error_text: str | None) -> None:
        reason = f" ({error_text})" if error_text else ""
        responses.append(
            MessageResponse(
                text=f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {failed_model} –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª–∞{reason}. –ü–æ—à–µ–ª —Å–ø—Ä–∞—à–∏–≤–∞—Ç—å –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å {next_model}."
            )
        )

    if request_type in {"text", "search", "search_previous", "consilium"} and ack_callback:
        try:
            await ack_callback()
        except Exception as exc:
            logger.warning("Failed to send ack message: %s", exc)

    if request_type == "help":
        capabilities = await get_capabilities()
        responses.extend(MessageResponse(text=part) for part in capabilities)
    elif request_type == "models_hint":
        responses.append(MessageResponse(text=MODELS_HINT_TEXT))
    elif request_type == "models_category":
        if content == "all":
            responses.extend(
                await send_models_by_request(
                    ["free", "large_context", "specialized", "paid"], MODELS_HINT_TEXT, max_items=None
                )
            )
        else:
            responses.extend(
                await send_models_by_request(
                    [content], CATEGORY_TITLES.get(content, "–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π:"), max_items=20
                )
            )
    elif request_type == "image":
        responses.append(MessageResponse(text="–ì–µ–Ω–µ—Ä–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ..."))
        image_url = await generate_image(content)
        if image_url:
            responses.append(MessageResponse(photo_url=image_url))
        else:
            responses.append(MessageResponse(text="–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."))
    elif request_type == "search":
        chat_id = str(chat_id)
        user_id = str(user_id)
        model_name = model or preferred_model or BOT_CONFIG["DEFAULT_MODEL"]

        responses.append(MessageResponse(text="–ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ..."))
        search_results = await search_web(content)

        prompt_with_search = (
            f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–ø—Ä–æ—Å–∏–ª –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é: '{content}'. –í–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ:\n\n{search_results}\n\n"
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–∞–π–¥–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –¥–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."
        )

        add_message(chat_id, user_id, "user", model_name, f"–ø–æ–≥—É–≥–ª–∏ {content}")

        response_text, used_model, context_info = await generate_text(
            prompt_with_search,
            model_name,
            chat_id,
            user_id,
            search_results=search_results,
            use_context=use_context,
            on_model_switch=notify_model_switch,
        )

        responses.extend(MessageResponse(text=notice) for notice in _build_context_guard_notices(context_info))

        add_message(chat_id, user_id, "assistant", used_model, response_text)

        header = _format_response_header(routed.user_routing_mode, context_info, used_model) if show_response_header else None
        reply_text = f"{header}\n\n{response_text}" if header else response_text
        responses.append(MessageResponse(text=reply_text))
    elif request_type == "search_previous":
        chat_id = str(chat_id)
        user_id = str(user_id)
        model_name = model or preferred_model or BOT_CONFIG["DEFAULT_MODEL"]

        history = get_history(chat_id, user_id, limit=10)

        previous_user_message: str | None = None
        previous_assistant_message: str | None = None

        for msg in history:
            if msg["role"] == "assistant" and not previous_assistant_message:
                previous_assistant_message = msg["text"]
            elif (
                msg["role"] == "user"
                and msg["text"].lower() not in ["–ø–æ–≥—É–≥–ª–∏", "–ø–æ–∏—â–∏"]
                and not previous_user_message
            ):
                previous_user_message = msg["text"]
                if previous_assistant_message:
                    break

        if not previous_user_message or not previous_assistant_message:
            responses.append(
                MessageResponse(
                    text=(
                        "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–∫–∞–∂–∏—Ç–µ, —á—Ç–æ –∏—Å–∫–∞—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä: '–ø–æ–≥—É–≥–ª–∏ –ø–æ–≥–æ–¥–∞ –≤ –ú–æ—Å–∫–≤–µ'"
                    )
                )
            )
            return responses

        responses.append(
            MessageResponse(
                text=f"–ò—â—É –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É –≤–æ–ø—Ä–æ—Å—É: '{previous_user_message}'..."
            )
        )

        search_prompt = (
            f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ä–∞–Ω–µ–µ —Å–ø—Ä–æ—Å–∏–ª: '{previous_user_message}'\n\n"
            f"–Ø –æ—Ç–≤–µ—Ç–∏–ª: '{previous_assistant_message}'\n\n"
            "–¢–µ–ø–µ—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç –Ω–∞–π—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∫—Ä–∞—Ç–∫–∏–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (2-5 —Å–ª–æ–≤) –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–∂–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç—å –∏–ª–∏ —É—Ç–æ—á–Ω–∏—Ç—å –º–æ–π –æ—Ç–≤–µ—Ç. –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ–∏—Å–∫–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–æ–º, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤."
        )

        search_query_response, _used_model, _context_info = await generate_text(
            search_prompt,
            model_name,
            None,
            None,
            use_context=False,
            on_model_switch=notify_model_switch,
        )
        search_query = search_query_response.strip().strip('"').strip("'")

        logger.info("Model formulated search query: '%s'", search_query)

        search_results = await search_web(search_query)

        final_prompt = (
            f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ä–∞–Ω–µ–µ —Å–ø—Ä–æ—Å–∏–ª: '{previous_user_message}'\n\n"
            f"–Ø —Ä–∞–Ω–µ–µ –æ—Ç–≤–µ—Ç–∏–ª: '{previous_assistant_message}'\n\n"
            f"–¢–µ–ø–µ—Ä—å —è –Ω–∞—à–µ–ª –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{search_query}':\n\n{search_results}\n\n"
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–∞–π–¥–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –¥–æ–ø–æ–ª–Ω–∏ –º–æ–π –ø—Ä–µ–¥—ã–¥—É—â–∏–π –æ—Ç–≤–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞."
        )

        add_message(chat_id, user_id, "user", model_name, "–ø–æ–≥—É–≥–ª–∏")

        response_text, used_model, context_info = await generate_text(
            final_prompt,
            model_name,
            chat_id,
            user_id,
            search_results=search_results,
            use_context=use_context,
            on_model_switch=notify_model_switch,
        )

        responses.extend(MessageResponse(text=notice) for notice in _build_context_guard_notices(context_info))

        add_message(chat_id, user_id, "assistant", used_model, response_text)

        header = _format_response_header(routed.user_routing_mode, context_info, used_model) if show_response_header else None
        reply_text = f"{header}\n\n{response_text}" if header else response_text
        responses.append(MessageResponse(text=reply_text))

    elif request_type == "consilium":
        chat_id = str(chat_id)
        user_id = str(user_id)

        models = suggested_models or await parse_models_from_message(content)

        if not models:
            models = await select_default_consilium_models()
            if not models:
                responses.append(
                    MessageResponse(
                        text="‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–æ–Ω—Å–∏–ª–∏—É–º–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–∫–∞–∑–∞—Ç—å –º–æ–¥–µ–ª–∏ —è–≤–Ω–æ."
                    )
                )
                return responses

        prompt = extract_prompt_from_consilium_message(content)

        if not prompt:
            responses.append(
                MessageResponse(
                    text="‚ùå –ù–µ —É–∫–∞–∑–∞–Ω –≤–æ–ø—Ä–æ—Å –¥–ª—è –∫–æ–Ω—Å–∏–ª–∏—É–º–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: –∫–æ–Ω—Å–∏–ª–∏—É–º: –≤–∞—à –≤–æ–ø—Ä–æ—Å"
                )
            )
            return responses

        status_message = MessageResponse(text=f"üè• –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç—ã –æ—Ç {len(models)} –º–æ–¥–µ–ª–µ–π...")
        responses.append(status_message)

        if BOT_CONFIG.get("CONSILIUM_CONFIG", {}).get("SAVE_TO_HISTORY", True):
            add_message(chat_id, user_id, "user", models[0], prompt)

        start_time = time.time()

        results = await generate_consilium_responses(prompt, models, chat_id, user_id)

        execution_time = time.time() - start_time

        formatted_messages = format_consilium_results(results, execution_time)

        if BOT_CONFIG.get("CONSILIUM_CONFIG", {}).get("SAVE_TO_HISTORY", True):
            for result in results:
                if result.get("success") and result.get("response"):
                    add_message(chat_id, user_id, "assistant", result.get("model"), result.get("response"))

        max_length = 4000
        for msg in formatted_messages:
            if len(msg) > max_length:
                parts: list[str] = []
                current_part = ""
                lines = msg.split("\n")

                for line in lines:
                    if len(current_part) + len(line) + 1 > max_length:
                        if current_part:
                            parts.append(current_part)
                        current_part = line + "\n"
                    else:
                        current_part += line + "\n"

                if current_part:
                    parts.append(current_part)

                for i, part in enumerate(parts):
                    if i == 0:
                        responses.append(MessageResponse(text=part))
                    else:
                        responses.append(
                            MessageResponse(
                                text=f"*(–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ {i+1}/{len(parts)})*\n\n{part}", parse_mode="Markdown"
                            )
                        )
            else:
                responses.append(MessageResponse(text=msg))

    elif request_type == "text":
        chat_id = str(chat_id)
        user_id = str(user_id)
        model_name = model or preferred_model or BOT_CONFIG["DEFAULT_MODEL"]
        add_message(chat_id, user_id, "user", model_name, content)

        response_text, used_model, context_info = await generate_text(
            content,
            model_name,
            chat_id,
            user_id,
            use_context=use_context,
            on_model_switch=notify_model_switch,
        )

        responses.extend(MessageResponse(text=notice) for notice in _build_context_guard_notices(context_info))

        add_message(chat_id, user_id, "assistant", used_model, response_text)

        header = _format_response_header(routed.user_routing_mode, context_info, used_model) if show_response_header else None
        reply_text = f"{header}\n\n{response_text}" if header else response_text
        responses.append(MessageResponse(text=reply_text))
    else:
        logger.warning("Unknown request type: %s", request_type)
        responses.append(MessageResponse(text="–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–∞—à –∑–∞–ø—Ä–æ—Å."))

    return responses


async def process_message_request(
    request: MessageProcessingRequest,
    ack_callback: Optional[Callable[[], Awaitable[None]]] = None,
    router_start_callback: Optional[Callable[[], Awaitable[None]]] = None,
    router_decision_callback: Optional[Callable[[RoutedRequest], Awaitable[bool]]] = None,
) -> List[MessageResponse]:
    """–û–±—â–∞—è –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ª—é–±—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º."""

    responses: List[MessageResponse] = []

    if not request.text:
        logger.debug("Received empty text, ignoring")
        return responses

    text = request.text
    chat_id = request.chat_id
    user_id = request.user_id
    bot_username = request.bot_username

    effective_text = text

    logger.info(
        "Processing message '%s' from user %s in chat %s",
        text,
        request.username or user_id,
        chat_id,
    )

    normalized_text = effective_text.strip()

    if normalized_text.lower() in _MODEL_PREFERENCE_RESET_KEYWORDS:
        set_preferred_model(chat_id, user_id, None)
        responses.append(
            MessageResponse(
                text=(
                    "üîÑ –í–µ—Ä–Ω—É–ª—Å—è –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏. –ß—Ç–æ–±—ã —Å–Ω–æ–≤–∞ –∑–∞–∫—Ä–µ–ø–∏—Ç—å –º–æ–¥–µ–ª—å, –Ω–∞–ø–∏—à–∏ '–æ—Ç–≤–µ—á–∞–π –≤—Å–µ–≥–¥–∞ —Å gpt'."
                )
            )
        )
        return responses

    model_preference_match = _MODEL_PREFERENCE_PATTERN.match(normalized_text)
    if model_preference_match:
        requested_model = model_preference_match.group(1).strip()
        resolved_model = _resolve_model_alias(requested_model)

        if not resolved_model:
            available_aliases = ", ".join(sorted(BOT_CONFIG.get("MODELS", {}).keys()))
            responses.append(
                MessageResponse(text=f"‚ùå –Ø –Ω–µ –∑–Ω–∞—é –º–æ–¥–µ–ª—å '{requested_model}'. –î–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: {available_aliases}.")
            )
            return responses

        set_preferred_model(chat_id, user_id, resolved_model)
        preferred_model = resolved_model
        responses.append(
            MessageResponse(
                text=(
                    "‚úÖ –ó–∞–ø–æ–º–Ω–∏–ª: –±—É–¥—É –æ—Ç–≤–µ—á–∞—Ç—å —á–µ—Ä–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –ø–æ–∫–∞ –Ω–µ –ø–æ–ø—Ä–æ—Å–∏—à—å –∏–Ω–∞—á–µ. "
                    "–ß—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π, –Ω–∞–ø–∏—à–∏ '–æ—Ç–≤–µ—á–∞–π –∫–∞–∫ –æ–±—ã—á–Ω–æ'."
                )
            )
        )
        return responses

    header_toggle = _normalize_header_toggle(effective_text)
    if header_toggle is not None:
        set_show_response_header(chat_id, user_id, header_toggle)
        reply = (
            "üõ† –¢–µ—Ö—à–∞–ø–∫–∞ –≤–∫–ª—é—á–µ–Ω–∞ –∏ –±—É–¥–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –Ω–∞–¥ –æ—Ç–≤–µ—Ç–∞–º–∏.\n"
            "–ß—Ç–æ–±—ã —Å–∫—Ä—ã—Ç—å, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ '—Å–∫—Ä—ã—Ç—å —à–∞–ø–∫—É' –∏–ª–∏ –∫–æ–º–∞–Ω–¥—É /header_off."
        )
        if not header_toggle:
            reply = (
                "ü´• –¢–µ—Ö—à–∞–ø–∫–∞ —Å–∫—Ä—ã—Ç–∞.\n"
                "–ß—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å –µ—ë, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ '–ø–æ–∫–∞–∑—ã–≤–∞–π —à–∞–ø–∫—É' –∏–ª–∏ –∫–æ–º–∞–Ω–¥—É /header_on."
            )

        responses.append(MessageResponse(text=reply))
        return responses

    routing_choice = _normalize_routing_choice(effective_text)
    if routing_choice:
        set_routing_mode(chat_id, user_id, routing_choice)
        mode_label = "–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π" if routing_choice == "rules" else "LLM"
        responses.append(
            MessageResponse(
                text=(
                    f"üîÄ –í–∫–ª—é—á—ë–Ω {mode_label} —Ä–æ—É—Ç–∏–Ω–≥ –¥–ª—è –≤–∞—à–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —ç—Ç–æ–º —á–∞—Ç–µ.\n"
                    f"–ß—Ç–æ–±—ã –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ '—Ä–æ—É—Ç–∏–Ω–≥ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏' –∏–ª–∏ '—Ä–æ—É—Ç–∏–Ω–≥ –ª–ª–º', –ª–∏–±–æ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–ª–µ—à-–∫–æ–º–∞–Ω–¥—ã /routing_rules –∏ /routing_llm."
                )
            )
        )
        return responses

    if _is_routing_status_request(effective_text):
        current_mode = get_routing_mode(chat_id, user_id) or BOT_CONFIG.get("ROUTING_MODE", "rules")
        mode_label = "–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π" if current_mode == "rules" else "LLM"
        responses.append(MessageResponse(text=f"üîé –¢–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º —Ä–æ—É—Ç–∏–Ω–≥–∞: {mode_label}."))
        return responses

    user_routing_mode = get_routing_mode(chat_id, user_id) or BOT_CONFIG.get("ROUTING_MODE", "rules")
    if user_routing_mode == "llm" and router_start_callback:
        try:
            await router_start_callback()
        except Exception as exc:
            logger.warning("Failed to send router start message: %s", exc)

    logger.info("Routing request (mode=%s): '%s'", user_routing_mode, effective_text)
    decision = await route_request(effective_text, bot_username, routing_mode=user_routing_mode)
    routed = _build_routed_request(decision, effective_text, user_routing_mode)
    logger.info(
        "Router resolved request to: %s, model: %s, use_context: %s, reason: %s",
        routed.request_type,
        routed.model,
        routed.use_context,
        routed.reason,
    )

    if user_routing_mode == "llm" and router_decision_callback:
        proceed = await router_decision_callback(routed)
        if not proceed:
            return responses

    return await execute_routed_request(request, routed, ack_callback=ack_callback)
