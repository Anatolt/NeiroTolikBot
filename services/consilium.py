import logging
import asyncio
import re
from typing import List, Dict, Optional
from config import BOT_CONFIG
from services.generation import (
    generate_text,
    _resolve_user_model_keyword,
    fetch_models_data,
    _is_free_pricing,
    _prepare_messages,
)

logger = logging.getLogger(__name__)
_MODEL_TOKEN_RE = re.compile(r"^[A-Za-z0-9_.:/-]+$")


def _is_model_token(token: str) -> bool:
    return bool(_MODEL_TOKEN_RE.match(token))


def _split_models_and_prompt(remaining: str) -> tuple[List[str], str]:
    models_part, prompt = remaining.split(":", 1)
    prompt = prompt.strip()
    tokens = re.split(r"[,;]\s*|\s+", models_part.strip())
    models = [t for t in (token.strip() for token in tokens) if t and _is_model_token(t)]
    return models, prompt


def _extract_consilium_remaining(text: str) -> str | None:
    text_lower = text.lower().strip()
    if text_lower.startswith("–∫–æ–Ω—Å–∏–ª–∏—É–º"):
        remaining = text[9:].strip()
    elif text_lower.startswith("/consilium"):
        remaining = text[10:].strip()
    else:
        return None

    if remaining.lower().startswith("—á–µ—Ä–µ–∑"):
        remaining = remaining[5:].strip()

    return remaining


def parse_consilium_request(text: str) -> tuple[List[str], str, bool]:
    remaining = _extract_consilium_remaining(text)
    if remaining is None:
        return [], "", False
    if ":" not in remaining:
        return [], "", False

    models_raw, prompt = _split_models_and_prompt(remaining)

    resolved_models = []
    for model_keyword in models_raw:
        resolved = _resolve_user_model_keyword(model_keyword)
        if resolved:
            resolved_models.append(resolved)
        else:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑—Ä–µ—à–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å, —á—Ç–æ–±—ã –ø—Ä–∏–∑–≤–∞—Ç—å –≤—Å–µ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏.
            logger.warning(f"Could not resolve model keyword, using as-is: {model_keyword}")
            resolved_models.append(model_keyword)

    return resolved_models, prompt, True

async def parse_models_from_message(text: str) -> List[str]:
    """
    –ü–∞—Ä—Å–∏—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    
    –ü—Ä–∏–º–µ—Ä—ã:
    - "–∫–æ–Ω—Å–∏–ª–∏—É–º —á–µ—Ä–µ–∑ chatgpt, claude, deepseek: –≤–æ–ø—Ä–æ—Å" -> ["openai/gpt-4-turbo", "anthropic/claude-3-haiku", "deepseek/deepseek-r1-distill-qwen-14b"]
    - "–∫–æ–Ω—Å–∏–ª–∏—É–º chatgpt claude" -> ["openai/gpt-4-turbo", "anthropic/claude-3-haiku"]
    - "–∫–æ–Ω—Å–∏–ª–∏—É–º: –≤–æ–ø—Ä–æ—Å" -> [] (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä)
    """
    models, _prompt, has_colon = parse_consilium_request(text)
    if not has_colon:
        return []
    return models


async def select_default_consilium_models() -> List[str]:
    """
    –í—ã–±–∏—Ä–∞–µ—Ç 3 —Ä–∞–∑–Ω—ã–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –∫–æ–Ω—Å–∏–ª–∏—É–º–∞.
    –ï—Å–ª–∏ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–æ–ª–±–µ–∫–∏.
    """
    selected_models: list[str] = []
    seen = set()
    excluded = set(BOT_CONFIG.get("EXCLUDED_MODELS", []))

    priority_order = BOT_CONFIG.get("PREFERRED_MODEL_ORDER", [])
    for alias in priority_order:
        resolved = _resolve_user_model_keyword(alias)
        if resolved and resolved not in seen and resolved not in excluded:
            selected_models.append(resolved)
            seen.add(resolved)
        if len(selected_models) >= 3:
            break

    # –ï—Å–ª–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –Ω–µ –Ω–∞–±—Ä–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    if len(selected_models) < 3:
        models_data = BOT_CONFIG.get("MODEL_CATALOG") or []

        if not models_data:
            try:
                models_data = await fetch_models_data()
                if models_data:
                    models_data = [m for m in models_data if m.get("id") not in excluded]
            except Exception as e:
                logger.warning(f"Failed to fetch models data: {e}")
                models_data = []

        free_models = []
        for model in models_data:
            model_id = model.get("id", "")
            if model_id in excluded:
                continue

            pricing = model.get("pricing", {}) if isinstance(model.get("pricing"), dict) else {}
            prompt_price = pricing.get("prompt")
            is_free = ":free" in model_id or _is_free_pricing(prompt_price)

            if is_free:
                free_models.append(model)

        free_models.sort(key=lambda m: m.get("context_length", 0) or 0, reverse=True)

        for model in free_models:
            model_id = model.get("id", "")
            if model_id and model_id not in seen:
                selected_models.append(model_id)
                seen.add(model_id)
                if len(selected_models) >= 3:
                    break

    # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–æ–ª–±–µ–∫–∏
    if len(selected_models) < 3:
        for model in BOT_CONFIG.get("FALLBACK_MODELS", []):
            if len(selected_models) >= 3:
                break
            if model and model not in seen and model not in excluded:
                selected_models.append(model)
                seen.add(model)

    # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º –ª—é–±—ã–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ MODELS
    if len(selected_models) < 3:
        for model_id in BOT_CONFIG.get("MODELS", {}).values():
            if len(selected_models) >= 3:
                break
            if model_id and model_id not in seen and model_id not in excluded and ":free" in model_id:
                selected_models.append(model_id)
                seen.add(model_id)

    # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º –ª—é–±—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ MODELS (–Ω–µ —Ç–æ–ª—å–∫–æ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ)
    if len(selected_models) < 3:
        for model_id in BOT_CONFIG.get("MODELS", {}).values():
            if len(selected_models) >= 3:
                break
            if model_id and model_id not in seen and model_id not in excluded:
                selected_models.append(model_id)
                seen.add(model_id)

    return selected_models[:3]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∞–∫—Å–∏–º—É–º 3 –º–æ–¥–µ–ª–∏


async def generate_single_model_response(
    prompt: str,
    model: str,
    chat_id: Optional[str],
    user_id: Optional[str],
    platform: Optional[str] = None,
    timeout: int = 60
) -> Dict:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å —Ç–∞–π–º–∞—É—Ç–æ–º.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –∏–ª–∏ –æ—à–∏–±–∫–æ–π.
    """
    try:
        enhanced_prompt = prompt + "\n\n–í–ê–ñ–ù–û: –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –º–∞–∫—Å–∏–º—É–º 100-150 —Å–ª–æ–≤). –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π markdown —Ä–∞–∑–º–µ—Ç–∫—É (**, ###, ``` –∏ —Ç.–¥.) - –ø–∏—à–∏ –ø—Ä–æ—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º. –û—Ç–≤–µ—á–∞–π –ø–æ —Å—É—â–µ—Å—Ç–≤—É –≤–æ–ø—Ä–æ—Å–∞."

        prepared_messages, guard_info = await _prepare_messages(
            enhanced_prompt, model, chat_id, user_id, None
        )

        response, used_model, context_info = await asyncio.wait_for(
            generate_text(
                enhanced_prompt,
                model,
                chat_id,
                user_id,
                prepared_messages=prepared_messages,
                context_info=guard_info,
                platform=platform,
            ),
            timeout=timeout
        )
        return {
            "model": used_model,
            "response": response,
            "success": True,
            "error": None,
            "context_notice": context_info,
        }
    except asyncio.TimeoutError:
        logger.error(f"Timeout generating response from model {model}")
        return {
            "model": model,
            "response": None,
            "success": False,
            "error": "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞"
        }
    except Exception as e:
        logger.error(f"Error generating response from model {model}: {str(e)}")
        return {
            "model": model,
            "response": None,
            "success": False,
            "error": str(e)[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—à–∏–±–∫–∏
        }


async def generate_consilium_responses(
    prompt: str,
    models: List[str],
    chat_id: Optional[str] = None,
    user_id: Optional[str] = None,
    platform: Optional[str] = None,
) -> List[Dict]:
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π.
    
    Args:
        prompt: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        models: –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        chat_id: ID —á–∞—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    """
    if not models:
        logger.warning("No models provided for consilium")
        return []

    # –ò–∑–±–∞–≤–ª—è–µ–º—Å—è –æ—Ç –¥—É–±–ª–µ–π, —á—Ç–æ–±—ã –æ–¥–Ω–∞ –∏ —Ç–∞ –∂–µ –º–æ–¥–µ–ª—å –Ω–µ –æ—Ç–≤–µ—á–∞–ª–∞ –¥–≤–∞–∂–¥—ã
    unique_models: list[str] = []
    seen: set[str] = set()
    for model in models:
        if model in seen:
            continue
        unique_models.append(model)
        seen.add(model)

    if len(unique_models) != len(models):
        logger.info(
            "Removed duplicate models from consilium request: %s -> %s",
            models,
            unique_models,
        )
    models = unique_models
    
    timeout = BOT_CONFIG.get("CONSILIUM_CONFIG", {}).get("TIMEOUT_PER_MODEL", 60)
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    tasks = [
        generate_single_model_response(prompt, model, chat_id, user_id, platform, timeout)
        for model in models
    ]
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
    processed_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logger.error(f"Exception in consilium task for model {models[i]}: {str(result)}")
            processed_results.append({
                "model": models[i],
                "response": None,
                "success": False,
                "error": f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {str(result)[:100]}"
            })
        else:
            processed_results.append(result)
    
    return processed_results


def _remove_markdown(text: str) -> str:
    """
    –£–¥–∞–ª—è–µ—Ç markdown —Ä–∞–∑–º–µ—Ç–∫—É –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    
    Args:
        text: –¢–µ–∫—Å—Ç —Å markdown —Ä–∞–∑–º–µ—Ç–∫–æ–π
    
    Returns:
        –¢–µ–∫—Å—Ç –±–µ–∑ markdown —Ä–∞–∑–º–µ—Ç–∫–∏
    """
    if not text:
        return text
    
    # –£–¥–∞–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (###, ##, #)
    text = re.sub(r'^#{1,6}\s+', '', text, flags=re.MULTILINE)
    
    # –£–¥–∞–ª—è–µ–º –∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç (**—Ç–µ–∫—Å—Ç**, __—Ç–µ–∫—Å—Ç__)
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
    text = re.sub(r'__([^_]+)__', r'\1', text)
    
    # –£–¥–∞–ª—è–µ–º –∫—É—Ä—Å–∏–≤ (*—Ç–µ–∫—Å—Ç*, _—Ç–µ–∫—Å—Ç_)
    text = re.sub(r'(?<!\*)\*([^*]+)\*(?!\*)', r'\1', text)
    text = re.sub(r'(?<!_)_([^_]+)_(?!_)', r'\1', text)
    
    # –£–¥–∞–ª—è–µ–º –∫–æ–¥ –±–ª–æ–∫–∏ (```–∫–æ–¥```)
    text = re.sub(r'```[\s\S]*?```', '', text)
    
    # –£–¥–∞–ª—è–µ–º –∏–Ω–ª–∞–π–Ω –∫–æ–¥ (`–∫–æ–¥`)
    text = re.sub(r'`([^`]+)`', r'\1', text)
    
    # –£–¥–∞–ª—è–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ª–∏–Ω–∏–∏ (---, ***)
    text = re.sub(r'^[-*]{3,}$', '', text, flags=re.MULTILINE)
    
    # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏ [—Ç–µ–∫—Å—Ç](url)
    text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
    
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = text.strip()
    
    return text


def format_consilium_results(results: List[Dict], execution_time: float = None) -> List[str]:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–Ω—Å–∏–ª–∏—É–º–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.
    
    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç –º–æ–¥–µ–ª–µ–π
        execution_time: –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ (–ø–µ—Ä–≤–æ–µ - –∑–∞–≥–æ–ª–æ–≤–æ–∫, –æ—Å—Ç–∞–ª—å–Ω—ã–µ - –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–µ–π)
    """
    if not results:
        return ["‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –æ—Ç –º–æ–¥–µ–ª–µ–π."]
    
    messages = []
    
    # –ü–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ - –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å –≤—Ä–µ–º–µ–Ω–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    header = "üè• –ö–æ–Ω—Å–∏–ª–∏—É–º –º–æ–¥–µ–ª–µ–π"
    if execution_time is not None and BOT_CONFIG.get("CONSILIUM_CONFIG", {}).get("SHOW_TIMING", True):
        header += f"\n‚è± –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.1f} —Å–µ–∫"
    messages.append(header)
    
    # –ö–∞–∂–¥—ã–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ - –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    for result in results:
        model = result.get("model", "unknown")
        success = result.get("success", False)
        
        if success:
            response = result.get("response", "")
            if response:
                # –£–¥–∞–ª—è–µ–º markdown –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º
                clean_response = _remove_markdown(response)
                notice = ""
                context_info = result.get("context_notice") or {}
                if context_info.get("summary_text"):
                    notice = "\n\n‚ÑπÔ∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω ‚Äî —Å–¥–µ–ª–∞–Ω–∞ –∫—Ä–∞—Ç–∫–∞—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏."
                elif context_info.get("trimmed_from_context"):
                    notice = "\n\n‚ÑπÔ∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω ‚Äî —á–∞—Å—Ç—å —Å—Ç–∞—Ä—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å–∫—Ä—ã—Ç–∞ –≤ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –∑–∞–ø—Ä–æ—Å–∞."
                elif context_info.get("warnings"):
                    notice = "\n\n‚ÑπÔ∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Ä–∞–∑–º–µ—Ä–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."
                messages.append(f"ü§ñ {model}:\n\n{clean_response}{notice}")
            else:
                messages.append(f"ü§ñ {model}:\n\n‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
        else:
            error = result.get("error", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞")
            messages.append(f"ü§ñ {model}:\n\n‚ùå –û—à–∏–±–∫–∞: {error}")
    
    return messages


def extract_prompt_from_consilium_message(text: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –∫–æ–Ω—Å–∏–ª–∏—É–º–æ–º.
    
    –ü—Ä–∏–º–µ—Ä—ã:
    - "–∫–æ–Ω—Å–∏–ª–∏—É–º: –∫–∞–∫–∞—è –ø–æ–≥–æ–¥–∞?" -> "–∫–∞–∫–∞—è –ø–æ–≥–æ–¥–∞?"
    - "–∫–æ–Ω—Å–∏–ª–∏—É–º —á–µ—Ä–µ–∑ chatgpt, claude: –æ–±—ä—è—Å–Ω–∏ –∫–≤–∞–Ω—Ç–æ–≤—É—é —Ñ–∏–∑–∏–∫—É" -> "–æ–±—ä—è—Å–Ω–∏ –∫–≤–∞–Ω—Ç–æ–≤—É—é —Ñ–∏–∑–∏–∫—É"
    - "–∫–æ–Ω—Å–∏–ª–∏—É–º chatgpt claude –∫–∞–∫–∞—è –ø–æ–≥–æ–¥–∞" -> "–∫–∞–∫–∞—è –ø–æ–≥–æ–¥–∞"
    """
    _models, prompt, has_colon = parse_consilium_request(text)
    if has_colon and prompt:
        return prompt
    return ""
